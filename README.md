# F1 Race Result Ranking & Prediction

Predict full race finishing order for Formula 1 using a **learning-to-rank** approach with strict **leakage-safe** feature engineering and **grouped cross-validation**. The project turns raw Ergast-style CSVs into features, trains rankers, evaluates per-race metrics (NDCG@k, Spearman, MAE vs **grid baseline**), and exports ready-to-use predictions.

---

## ğŸ”¥ Why this project is interesting (for reviewers/recruiters)

- **End-to-end ML system**: ingestion â†’ feature store â†’ CV â†’ training â†’ evaluation â†’ artifacts.
- **Learning-to-Rank**: models optimize ranking quality per race (e.g., LightGBM `lambdarank`).
- **No data leakage**: driver/constructor rolling stats are shifted; CV grouped by race/season.
- **Baselines that matter**: compare against â€œgrid = finishâ€ and season-naive baselines.
- **Reproducible runs**: every run logs config + metrics under `runs/exp/<timestamp>`.
- **Modular code**: clean `src/f1pred/*` with tests and configs that can plug in new features.

---

## ğŸ§± Repository layout

â”œâ”€ configs/
â”‚ â”œâ”€ base.yml # shared defaults (paths, features, CV, common params)
â”‚ â”œâ”€ rexp_after_quali.yml # run config when qualifying grid is known
â”‚ â””â”€ rexp_before_quali.yml # run config when only practice/priors are known
â”œâ”€ data/
â”‚ â”œâ”€ raw/ # drop CSVs here (see â€œDataâ€ section)
â”‚ â”œâ”€ interim/ # cached/intermediate parquet files
â”‚ â””â”€ processed/ # model-ready features per race
â”œâ”€ runs/
â”‚ â””â”€ exp// # config.yaml, metrics.json, cv_metrics.json, final_model.joblib
â”œâ”€ src/
â”‚ â””â”€ f1pred/
â”‚ â”œâ”€ init.py
â”‚ â”œâ”€ data_ingest.py
â”‚ â”œâ”€ build_features.py
â”‚ â”œâ”€ train.py # python -m f1pred.train -c configs/base.yml
â”‚ â”œâ”€ predict.py # python -m f1pred.predict -c ...
â”‚ â”œâ”€ evaluate.py
â”‚ â””â”€ utils.py
â”œâ”€ tests/
â”‚ â”œâ”€ test_no_leakage.py
â”‚ â””â”€ test_schemas.py
â””â”€ README.md

ğŸ“ Data

Place Ergast-style CSVs into data/raw/. Files commonly used here:
â€¢ circuits.csv
â€¢ constructors.csv
â€¢ constructor_results.csv
â€¢ constructor_standings.csv
â€¢ drivers.csv
â€¢ driver_standings.csv
â€¢ lap_times.csv
â€¢ pit_stops.csv
â€¢ qualifying.csv
â€¢ races.csv
â€¢ results.csv

â–¶ï¸ Train

# Example: default experiment

python -m f1pred.train -c configs/base.yml

# After-quali setup (grid included)

python -m f1pred.train -c configs/rexp_after_quali.yml

# Before-quali setup (no grid features)

python -m f1pred.train -c configs/rexp_before_quali.yml

Outputs (per run) in runs/exp/<timestamp>/:
â€¢ config.yaml (frozen copy)
â€¢ final_model.joblib
â€¢ metrics.json (global metrics)
â€¢ cv_metrics.json (per fold)
â€¢ optional: feature_importance.csv, oof_predictions.parquet

âœ… Tests
pytest -q
Typical guards included:
â€¢ No leakage in rolling features (first race per driver has NaNs on rolling stats).
â€¢ Per-race ranking sanity (each race should have multiple unique pred_rank values).
â€¢ Schema checks for required columns & dtypes.
